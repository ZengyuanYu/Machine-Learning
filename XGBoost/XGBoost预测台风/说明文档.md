## XGBoost预测台风

### 数据预处理和训练集测试集分离
- 使用Python包读取MATLAB文件到内存中：
```
import scipy.io as sio

matfile = 'hgt.mat'
data = sio.loadmat(matfile)
```

将文件中数据存放到一个列表中：
```
hgt_list = data['hgt_yy']
```

- 以50:16的比例分离训练集和测试集
```
import csv

with open("X_50.csv", "w") as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['key{}'.format(i) for i in range(498)])
    for j in range(50):
        writer.writerow(list(hgt_list[j]) 
                        + list(sst_list[j]) 
                        + list(vort_list[j]))
```

### 使用XGBoost对建模并计算准确分数
**由数据预处理将数据集分为训练集X_50.csv,Y_50.csv和X_16.csv,Y_16.csv**

- GBDTTrain(X, y)
对数据进行训练
参数：训练集的数据，训练集的标签
返回值：训练准确度的数std,以及XGBoost模型clf
**具体步骤如下**

	-1.将*X_50.csv*进行sklearn.split按照一定比例分离，构建XGBoost回归模型，对三个参数进行赋值覆盖默认，即学习率，树枝数，树深。
	-2.用模型对训练数据集进行训练，然后使用模型.predict()对分离出来的测试集进行测试得到标签。
	-3.将真实标签和预测标签进行`r2_score`计算回归准确率分数。
	-4.返回分数和创建的模型

- GBDTest(X, y, clf)
对训练好的模型进行验证
参数：测试集的数据，测试集的标签，训练好的模型
返回值：测试准确度的数std

**具体步骤如下**

	-1.将*X_16.csv*已经对应的标签和训练模型送入到函数中，使用`predict（）`函数对其进行预测
	-2.预测标签和真实标签一同送入到`r2_score`中预测分数
	-3.返回预测分数

- 结果可视化
见附件图片*train_labels*和*test_labels*

- 运行
python XGBoost.py

### 我的一些理解
通过手动调参，改变三个参数的组合，现在得到的最优解为train: 0.47 test: 0.42
**不同参数组合的结果**

No.|learning_rate|n_estimators|max_depth|train_score|test_score
-|:-:|:-:|:-:|:-:|-:
1|0.2|500|16|0.387|0.036
2|0.2|500|8|0.387|0.036
3|0.2|500|3|0.151|0.192
4|0.2|400|16|0.387|0.036
5|0.2|400|8|0.387|0.036
6|0.2|400|3|0.151|0.192
7|0.1|500|16|0.440|0.301
8|0.1|500|8|0.440|0.301
9|0.1|500|3|0.470|0.426
10|0.1|400|16|0.440|0.301
11|0.1|400|8|0.440|0.301
12|0.1|400|3|0.470|0.426

根据我个人的理解现在按照这个回归的思路去做预测，相对于498个因子影响一个输出来说，66个数据太少，哪怕是做线性回归，一般得到很好的效果都需要100个（x,y）的数据点，况且它是一个x，而台风预测是相同权重的498个x。而且这个预测的分数是<=1的数值，下一步的我需要做的是正确率评估，查找`sklearn`里面的一些验证方法去做，现在已有一些思路。
